{
 "metadata": {
  "name": "",
  "signature": "sha256:9547618cb85386ca69caa51f825bbc77c470fff0fa4deba2d8de745f73cec49d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from mvpa2.suite import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path_LA = os.path.join('/home', 'brain', 'Desktop', 'LAstudy')\n",
      "dhandle = OpenFMRIDataset(path_LA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "task = 1\n",
      "model = 1\n",
      "subj = 1\n",
      "run_datasets = []\n",
      "for run_id in dhandle.get_task_bold_run_ids(task)[subj]:\n",
      "    run_events = dhandle.get_bold_run_model(model, subj, run_id)\n",
      "    run_ds = dhandle.get_bold_run_dataset(subj, task, run_id, chunks = run_id - 1, flavor = 'preproc',\n",
      "                                          mask = path_LA+'/sub%03d/mask/mask.nii.gz'%(subj))\n",
      "    run_ds.sa['targets'] = events2sample_attr(run_events, run_ds.sa.time_coords, noinfolabel='rest')\n",
      "    run_datasets.append(run_ds)\n",
      " \n",
      "fds = vstack(run_datasets, a=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARNING: Detected incorrect (nan) scl_ fields. Resetting to scl_slope=1.0 and scl_inter=0.0\n",
        " * Please note: warnings are printed only once, but underlying problem might occur many times *\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly_detrend(fds, polyord=1, chunks_attr='chunks')\n",
      "zscore(fds, param_est=('targets', ['rest']))\n",
      "fds = fds[fds.sa.targets != 'rest']\n",
      "fds.sa['runtype'] = fds.sa.chunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseclf = LinearCSVMC()\n",
      "metaclf = MappedClassifier(baseclf, SVDMapper())\n",
      "cvte = CrossValidation(metaclf, NFoldPartitioner())\n",
      "cv_results = cvte(fds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " print np.mean(cv_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.171882543705\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s say we have heard rumors that only the first two dimensions of the space spanned by the SVD vectors cover the \u201cinteresting\u201d variance and the rest is noise. We can easily check that with an appropriate mapper:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mapper = ChainMapper([SVDMapper(), StaticFeatureSelection(slice(None, 2))])\n",
      "metaclf = MappedClassifier(baseclf, mapper)\n",
      "cvte = CrossValidation(metaclf, NFoldPartitioner())\n",
      "cv_results = cvte(fds)\n",
      "print np.mean(cv_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.173344118434\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Well, obviously the discarded components cannot only be noise, since the error is substantially increased. But maybe it is the classifier that cannot deal with the data. Since nothing in this code is specific to the actual classification algorithm we can easily go back to the kNN classifier that has served us well in the past."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseclf = kNN(k=1, dfx=one_minus_correlation, voting='majority')\n",
      "mapper = ChainMapper([SVDMapper(), StaticFeatureSelection(slice(None, 2))])\n",
      "metaclf = MappedClassifier(baseclf, mapper)\n",
      "cvte = CrossValidation(metaclf, NFoldPartitioner())\n",
      "cv_results = cvte(fds)\n",
      "print np.mean(cv_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.473191503307\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Oh, that was even worse. We would have to take a closer look at the data to figure out what is happening here."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}