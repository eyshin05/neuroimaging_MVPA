{
 "metadata": {
  "name": "",
  "signature": "sha256:9746daad9cf481b69370eac375b00bc6433fb1ce5860d9465d4e36c7782c0d32"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "from mvpa2.suite import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path_LA = os.path.join('/home', 'brain', 'Desktop', 'LAstudy')\n",
      "dhandle = OpenFMRIDataset(path_LA)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "task = 1\n",
      "model = 1\n",
      "subj = 2\n",
      "run_datasets = []\n",
      "for run_id in dhandle.get_task_bold_run_ids(task)[subj]:\n",
      "    run_events = dhandle.get_bold_run_model(model, subj, run_id)\n",
      "    run_ds = dhandle.get_bold_run_dataset(subj, task, run_id, chunks = run_id - 1, flavor = 'preproc',\n",
      "                                          mask = path_LA+'/sub%03d/mask/mask.nii.gz'%(subj))\n",
      "    run_ds.sa['targets'] = events2sample_attr(run_events, run_ds.sa.time_coords, noinfolabel='rest')\n",
      "    run_datasets.append(run_ds)\n",
      " \n",
      "fds = vstack(run_datasets, a=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly_detrend(fds, polyord=1, chunks_attr='chunks')\n",
      "zscore(fds, param_est=('targets', ['rest']))\n",
      "fds = fds[fds.sa.targets != 'rest']\n",
      "fds.sa['runtype'] = fds.sa.chunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A simple approach would be to compute a full-brain ANOVA and only go with the voxels that show some level of variance between categories. ... However, PyMVPA offers a more convenient way \u2013 feature selectors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel = SensitivityBasedFeatureSelection(OneWayAnova(), \n",
      "                                        FixedNElementTailSelector(500, mode='select', tail='upper'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fsel.train(fds)\n",
      "fds_p = fsel(fds)\n",
      "print fds_p.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(621, 500)\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fds.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "(621, 76542)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "baseclf = LinearCSVMC()\n",
      "metaclf = MappedClassifier(baseclf, SVDMapper())\n",
      "cvte = CrossValidation(metaclf, NFoldPartitioner(), enable_ca = ['stats'])\n",
      "cv_results = cvte(fds)\n",
      "print np.mean(cv_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.248779909974\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cvte.ca.stats.as_string(description=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------.\n",
        "predictions\\targets    0      1\n",
        "            `------  -----  -----   P'    N'  FP FN  PPV  NPV  TPR  SPC  FDR MCC  F1   AUC\n",
        "         0            224    68    292   329  68 86 0.77 0.74 0.72 0.78 0.23 0.5 0.74 0.86\n",
        "         1            86     243   329   292  86 68 0.74 0.77 0.78 0.72 0.26 0.5 0.76 0.86\n",
        "Per target:          -----  -----\n",
        "         P            310    311\n",
        "         N            311    310\n",
        "         TP           224    243\n",
        "         TN           243    224\n",
        "Summary \\ Means:     -----  ----- 310.5 310.5 77 77 0.75 0.75 0.75 0.75 0.25 0.5 0.75 0.86\n",
        "       CHI^2         159.9 p=1.9e-34\n",
        "        ACC          0.75\n",
        "        ACC%         75.2\n",
        "     # of sets         3\n",
        "\n",
        "Statistics computed in 1-vs-rest fashion per each target.\n",
        "Abbreviations (for details see http://en.wikipedia.org/wiki/ROC_curve):\n",
        " TP : true positive (AKA hit)\n",
        " TN : true negative (AKA correct rejection)\n",
        " FP : false positive (AKA false alarm, Type I error)\n",
        " FN : false negative (AKA miss, Type II error)\n",
        " TPR: true positive rate (AKA hit rate, recall, sensitivity)\n",
        "      TPR = TP / P = TP / (TP + FN)\n",
        " FPR: false positive rate (AKA false alarm rate, fall-out)\n",
        "      FPR = FP / N = FP / (FP + TN)\n",
        " ACC: accuracy\n",
        "      ACC = (TP + TN) / (P + N)\n",
        " SPC: specificity\n",
        "      SPC = TN / (FP + TN) = 1 - FPR\n",
        " PPV: positive predictive value (AKA precision)\n",
        "      PPV = TP / (TP + FP)\n",
        " NPV: negative predictive value\n",
        "      NPV = TN / (TN + FN)\n",
        " FDR: false discovery rate\n",
        "      FDR = FP / (FP + TP)\n",
        " MCC: Matthews Correlation Coefficient\n",
        "      MCC = (TP*TN - FP*FN)/sqrt(P N P' N')\n",
        " F1 : F1 score\n",
        "      F1 = 2TP / (P + P') = 2TP / (2TP + FP + FN)\n",
        " AUC: Area under (AUC) curve\n",
        " CHI^2: Chi-square of confusion matrix\n",
        " LOE(ACC): Linear Order Effect in ACC across sets\n",
        " # of sets: number of target/prediction sets which were provided\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "double-dipping procedure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}